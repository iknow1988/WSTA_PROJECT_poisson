{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group: poisson\n",
    "\n",
    "Student Name: Kazi Abir Adnan\n",
    "Student ID: 940406\n",
    "\n",
    "Student Name: Daniel Gil\n",
    "Student ID: 905923\n",
    "\n",
    "Python version used: 3.5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Necessary files and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import groupby\n",
    "from math import log\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tag.stanford import CoreNLPNERTagger\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer,PunktTrainer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Utility functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving object\n",
    "def save_obj(obj,name):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Load saved object file\n",
    "def load_obj(name):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Accuracy Report\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"Accuracy:\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))\n",
    "\n",
    "#returns lemmatized word\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,wn.NOUN)\n",
    "    if (lemma == word):\n",
    "        lemma = lemmatizer.lemmatize(word,wn.VERB)\n",
    "    return lemma\n",
    "\n",
    "# GET TF-IDF score of word in paragraph context\n",
    "def get_tfidf(word,tf_idf,paragraph):\n",
    "    result = 0.0\n",
    "    found = False\n",
    "    values = tf_idf[word]\n",
    "    for value in values:\n",
    "        if value[0] == paragraph:\n",
    "            found  = True\n",
    "            result = value [1]\n",
    "            break\n",
    "    return result\n",
    "\n",
    "# GET sentence vector on paragraph context using Gensim word2vec model and TF-IDF score\n",
    "def sentence_vector(word2vec_model, sentence,tf_idf,paragraph,qt):\n",
    "    word_vectors = word2vec_model.wv\n",
    "    doc = []\n",
    "    tokenized_word = nltk.word_tokenize(sentence)\n",
    "    for word in tokenized_word:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            word = lemmatize(word)\n",
    "            if word in word_vectors.vocab:\n",
    "                doc.append(word)\n",
    "    score = []\n",
    "    for word in doc:\n",
    "        score.append(get_tfidf(word,tf_idf,paragraph)*word_vectors[word])\n",
    "\n",
    "    if qt in word_vectors.vocab:\n",
    "        score.append(word_vectors[qt])\n",
    "    \n",
    "    return np.mean(score, axis=0)\n",
    "\n",
    "# get the word counter for a paragraph\n",
    "def get_paragraph_words(paragraph):\n",
    "    terms = Counter()\n",
    "    for sentence in nltk.sent_tokenize(paragraph):\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            if token not in stopwords: \n",
    "                token = lemmatize(token.lower())\n",
    "                terms[token] = terms.get(token, 0) +  1\n",
    "    return terms\n",
    "\n",
    "# Get the document dictionary of paragraph term frequencies\n",
    "def get_document_paragraphs_term(ds_documents):\n",
    "    document_term={}\n",
    "    for index, row in ds_documents.iterrows():\n",
    "        terms={}\n",
    "        for paragraph_id, paragraph in enumerate(row['text']):\n",
    "            terms[paragraph_id]=get_paragraph_words(paragraph)\n",
    "        document_term[row['docid']]=terms\n",
    "    return document_term\n",
    "\n",
    "# Get the term frequency distribution on paragraph of a document\n",
    "def compute_paragraph_freqs(doc_term_freqs):\n",
    "    doc_dic = {}\n",
    "    for key, value in doc_term_freqs.items():\n",
    "        dfs = Counter()\n",
    "        for passage_id,tfs in value.items():\n",
    "            for term in tfs.keys():\n",
    "                dfs[term] += 1\n",
    "        doc_dic[key] = dfs\n",
    "        \n",
    "    return doc_dic\n",
    "\n",
    "# Get the total count of words in a paragraph\n",
    "def count_words(freqs):\n",
    "    p_count = 0\n",
    "    for counter in freqs.values():\n",
    "        p_count+=sum(counter.values())\n",
    "    return p_count\n",
    "\n",
    "# Get name entities of sentence using NLTK\n",
    "def get_Name_Entity_NLTK(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        ne_chunked_sents = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "        result = []\n",
    "\n",
    "        for tagged_tree in ne_chunked_sents:\n",
    "\n",
    "            if hasattr(tagged_tree, 'label'):\n",
    "                entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "                entity_type = tagged_tree.label() # get NE category\n",
    "                result.append((entity_name, entity_type))\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Get name entities of sentence using Stanford CORENLTK\n",
    "def get_Name_Entity_Sentence(sentence):\n",
    "    st = CoreNLPNERTagger(url='http://localhost:9000')\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    result = []\n",
    "    \n",
    "    for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "       if tag != \"O\":\n",
    "            word = \" \".join(w for w, t in chunk)\n",
    "            result.append((word.lower(), tag))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Add Name Entity column on dataframe based on the feature given\n",
    "def addNameEntity(df,feature,func):\n",
    "    if 'NE'+\"_\"+feature in df:\n",
    "        df = df.drop('NE'+\"_\"+feature, axis=1)\n",
    "    df[\"NE\"+\"_\"+feature] = func(df[feature])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# GET Question keyword on question\n",
    "def get_question_type(question):\n",
    "    found  = False\n",
    "    result = 'other'\n",
    "    question_tokens = nltk.word_tokenize(question)\n",
    "    for token in question_tokens:\n",
    "        if token in questionwords:\n",
    "            found = True\n",
    "            result = token\n",
    "    if not found:\n",
    "        for token in question_tokens:\n",
    "            if token in passiveQuestions:\n",
    "                found = True\n",
    "                result = token\n",
    "\n",
    "    return result\n",
    "\n",
    "# GET POS paragraph\n",
    "def get_POS_paragraph(data):\n",
    "    result = []\n",
    "    tokenized_sentence = nltk.sent_tokenize(data)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result.append(get_POS(sentence))\n",
    "    return result\n",
    "\n",
    "# GET POS sentence\n",
    "def get_POS(data):\n",
    "    tokenized_text = tokenizer.tokenize(data)\n",
    "    nltk_tag = nltk.pos_tag(tokenized_text)\n",
    "    stan_tag = stanford_tagger.tag(tokenized_text)\n",
    "    return stan_tag\n",
    "\n",
    "# Get keywords on sentence\n",
    "def get_keyword(data):\n",
    "    result = []\n",
    "    sentence=data\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    for text,pos in tagged:\n",
    "        text = lemmatize(text.lower())\n",
    "        if text not in stopwords:\n",
    "            if pos in POS:\n",
    "                result.append(text)\n",
    "                \n",
    "    return result\n",
    "\n",
    "# Get keywords on paragraph\n",
    "def get_keyword_paragraph(data):\n",
    "    results=[]\n",
    "    tokenized_sentence = tokenizer.tokenize(data)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Get keywords on document\n",
    "def get_keyword_all(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Add keyword column on  dataframe based on feature\n",
    "def add_keywords(df,feature):\n",
    "    if 'keywords'+\"_\"+feature in df:\n",
    "        df = df.drop('keywords'+\"_\"+feature, axis=1)\n",
    "    df['keywords'+\"_\"+feature]=get_keyword_all(df[feature])\n",
    "    return df\n",
    "\n",
    "# get common keywords on question and a sentence\n",
    "def get_number_of_common_kewyords(question_keywords,answer_sentence_keywords):\n",
    "    sum_keywords=0\n",
    "    for qkey in question_keywords:\n",
    "        if qkey in answer_sentence_keywords:\n",
    "            sum_keywords+=1\n",
    "    \n",
    "    return sum_keywords\n",
    "\n",
    "# BOW extraction for passages and questions\n",
    "def get_passages_bow(passages):\n",
    "    passage_bow={}\n",
    "    for passage in passages:\n",
    "        for token in nltk.word_tokenize(passage):\n",
    "            if token not in stopwords: \n",
    "                word=lemmatize(token.lower())\n",
    "                passage_bow[word] = passage_bow.get(word, 0) +  1\n",
    "    \n",
    "    return passage_bow\n",
    "\n",
    "# BOW extraction for sentences\n",
    "def get_sentences_bow(sentences):\n",
    "    sentence_bow={}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            if token not in stopwords:\n",
    "                word=lemmatize(token.lower())\n",
    "                sentence_bow[word] = sentence_bow.get(word, 0) +  1\n",
    "    \n",
    "    return sentence_bow\n",
    "\n",
    "# Get question BOW\n",
    "def get_question_bow(question):\n",
    "    question_bow={}\n",
    "    question_bow[get_question_type(question)]=1\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=lemmatize(token.lower())\n",
    "            question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow\n",
    "\n",
    "# Get common BOW of question and answer keywords\n",
    "def get_training_question_bow(question,keywords,qt):\n",
    "    question_bow={}\n",
    "    question_bow[qt]=1\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=lemmatize(token.lower())\n",
    "            if word in keywords:\n",
    "                question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow\n",
    "\n",
    "#Get training dataset features\n",
    "def get_feature_questions(questions, keywords,qt):\n",
    "    qs = []\n",
    "    for i,question in enumerate(questions):\n",
    "        q_bow = get_training_question_bow(question,keywords,qt[i])\n",
    "        qs.append(q_bow)\n",
    "        \n",
    "    return qs\n",
    "\n",
    "#Get testing dataset features\n",
    "def get_feature_question(question, keywords,qt):\n",
    "#     print(question,keywords,qt)\n",
    "    q_bow = get_training_question_bow(question,keywords,qt)\n",
    "      \n",
    "    return q_bow\n",
    "\n",
    "# get a list of paragraphs ordered by relevance on the question\n",
    "def query_vsm(query, index):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator\n",
    "\n",
    "def get_number_of_common_entities(ans,question):\n",
    "    question_NE = set(x[1] for x in question)\n",
    "    answer_NE = set(x[1] for x in ans)\n",
    "    sum_keywords=0\n",
    "    for qkey in question_NE:\n",
    "        if qkey in answer_NE:\n",
    "            sum_keywords+=1\n",
    "#     print(ans,question)\n",
    "    return sum_keywords\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading saved objects</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=pd.read_pickle(\"project_files/df_training.pkl\")\n",
    "df_devel=pd.read_pickle('project_files/df_devel.pkl')\n",
    "df_docs=pd.read_json('project_files/documents.json')\n",
    "df_testing=pd.read_pickle('project_files/df_testing.pkl')\n",
    "\n",
    "question_learning_dataset = df_training[df_training.answer_type.notnull()]\n",
    "question_devel_dataset = df_devel[df_devel.answer_type.notnull()]\n",
    "\n",
    "NER_corpus=load_obj('ner_corpus')\n",
    "random_forest_model = load_obj('random_forest')\n",
    "vectorizer = load_obj('Vectorizer')\n",
    "answer_keywords = load_obj('answer_keywords')\n",
    "gensim_model = Word2Vec.load(\"obj/gensim_model\")\n",
    "gensim_features = load_obj('features')\n",
    "gensim_labels = load_obj('labels')\n",
    "vsm_inverted_index_corpus = load_obj('vsm_inverted_index_corpus')\n",
    "vsm_inverted_index_all = load_obj('vsm_inverted_index_all')\n",
    "doc_corpus_term_frquency = load_obj('doc_corpus_term_frquency')\n",
    "doc_corpus_inverse_term_frquency = load_obj('doc_corpus_inverse_term_frquency')\n",
    "punk_tokenizer = load_obj('punk_tokenizer')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Initilaizing some global objects</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stanford_tagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "\n",
    "POS = set([\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"CD\",\"JJ\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]) \n",
    "\n",
    "punk_tokenizer._params.abbrev_types.add('ii')\n",
    "punk_tokenizer._params.abbrev_types.add('dr')\n",
    "\n",
    "questionwords = set([\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\",\"whats\",\"what's\",\"whos\"])\n",
    "passiveQuestions = set([\"can\", \"could\", \"would\", \n",
    "                   \"was\", \"were\",\"am\",\"is\", \"are\", \"will\",\"shall\",\n",
    "                   \"did\",\"do\",\"does\",\n",
    "                   \"had\", \"have\",\"has\",\n",
    "                   \"as\",\"that\",\"in\",\n",
    "                   \"give an example\",\"name\"])\n",
    "label={'NUMBER':'NUMBER','DATE':'TIME','PERSON':'PERSON','ORGANIZATION':'ORGANIZATION',\n",
    "      'MISC':'MISC','MONEY':'NUMBER','COUNTRY':'LOCATION',\n",
    "      'PERCENT':'NUMBER','TITLE':'PERSON','STATE_OR_PROVINCE':'LOCATION',\n",
    "      'CAUSE_OF_DEATH':'NAME','DURATION':'TIME','CRIMINAL_CHARGE':'NAME',\n",
    "       'CITY':'LOCATION','RELIGION':'NAME','SET':'TIME','NATIONALITY':'NAME',\n",
    "       'IDEOLOGY':'NAME','ORDINAL':'NUMBER','TIME':'TIME','URL':'ORGANIZATION',None:'OTHER','LOCATION':'LOCATION'}\n",
    "\n",
    "df_result_devel=pd.DataFrame(columns=['id','question','paragraph','retrieved paras','predicted_paragraph',\n",
    "                                      'paragraph_found','sentence','predicted_sentence','predicted_sentence_id',\n",
    "                                      'sentence_found','answer','predicted_answer',\n",
    "                                      'NE_question','label','predicted_paragraph_final'])\n",
    "\n",
    "df_result_test=pd.DataFrame(columns=['id','question','paragraph','retrieved paras','predicted_paragraph',\n",
    "                                      'paragraph_found','sentence','predicted_sentence','predicted_sentence_id',\n",
    "                                      'sentence_found','answer','predicted_answer','NE_question',\n",
    "                                     'label','predicted_paragraph_final'])\n",
    "\n",
    "df_best_sentences=pd.DataFrame(columns=['doc_id','para_id','original','sentence_id','sentence_text','score','sentence'])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing training dataset and understanding tweeked answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Don't Run any of this functions in this section. This is only to analyze the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Tokenizer PUNKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for index, doc in df_docs.iterrows():\n",
    "    print (index)\n",
    "    for paragraph in doc['text']:\n",
    "        text = text + paragraph+\" \"\n",
    "\n",
    "punk_trainer = PunktTrainer()\n",
    "punk_trainer.INCLUDE_ALL_COLLOCS = True\n",
    "punk_trainer.train(text)\n",
    "punk_trainer = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_find(sentence,answer):\n",
    "    s_t = set(x.lower() for x in nltk.word_tokenize(sentence))\n",
    "    a_t = nltk. word_tokenize(answer)\n",
    "    for a in a_t:\n",
    "        if a.lower() not in s_t:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def answer_find2(sentence,answer):\n",
    "    s_t = set(x.lower().replace('-',' ').replace('\\'','') for x in nltk.word_tokenize(sentence.replace('-',' ')))\n",
    "    a_t = nltk.word_tokenize(answer.replace('-',' '))\n",
    "    for a in a_t:\n",
    "        if a.lower() not in s_t:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "sentences = []\n",
    "count = 0\n",
    "for index,row in df_training.iterrows():\n",
    "    p_id = row['answer_paragraph']\n",
    "    doc_id = row['docid']\n",
    "    paragraph = df_docs.iloc[doc_id]['text'][p_id]\n",
    "    found = False\n",
    "    for sent in tokenizer.tokenize(paragraph):\n",
    "        if(answer_find(sent,row['text'])):\n",
    "            sentences.append(sent)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        for sent in tokenizer.tokenize(paragraph):\n",
    "            if(answer_find2(sent, row['text'])):\n",
    "                sentences.append(sent)\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        for sent in nltk.sent_tokenize(paragraph):\n",
    "            if row['text'].lower() in sent.lower():\n",
    "                sentences.append(sent)\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        count = count + 1\n",
    "        print (index,row['text'])\n",
    "        print(paragraph)\n",
    "        sentences.append(None)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set some predefined question type to entity type (Ex. Who/Whom/Who's -> PERSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_questions(question_type,entity_type):\n",
    "    count = 0\n",
    "    for index, row in df_training.iterrows():\n",
    "        ne_answer = row['NE_text']\n",
    "        question = row['question']\n",
    "        tokenized_text = tokenizer.tokenize(question)\n",
    "        for text in tokenized_text:\n",
    "            if (text.lower() == question_type.lower()):\n",
    "                answer = row['text']\n",
    "                if(len(ne_answer))<1:\n",
    "                    print (index,question,answer)\n",
    "                    count = count + 1\n",
    "#                     df_training.set_value(index,'NE_text',get_Name_Entity_StanfordCoreNLP_sentence_caseless(answer))\n",
    "                break\n",
    "    print (count)\n",
    "    \n",
    "questionwords = set([\"what\", \"when\", \"why\", \"how\", \"whose\", \"which\",\"whats\",\"what's\"])\n",
    "\n",
    "print_questions(\"who\",\"PERSON\")\n",
    "print_questions(\"where\",\"LOCATION\")\n",
    "print_questions(\"whom\",\"PERSON\")\n",
    "print_questions(\"whose\",\"PERSON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question type set to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionwords = set([\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\",\"whats\",\"what's\",\"whos\"])\n",
    "passiveQuestions = [\"can\", \"could\", \"would\", \n",
    "                    \"was\", \"were\",\"am\",\"is\", \"are\", \"will\",\"shall\",\n",
    "                    \"did\",\"do\",\"does\",\n",
    "                    \"had\", \"have\",\"has\",\n",
    "                    \"as\",\"that\",\"in\",\n",
    "                    \"give an example\",\"name\"]\n",
    "question_types = []\n",
    "count  = 0\n",
    "for index,row in df_testing.iterrows():\n",
    "    sentence = row[\"question\"]\n",
    "    question_type = 'other'\n",
    "    found = False\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    for text in tokenized_text:\n",
    "        if text.lower() in questionwords:\n",
    "            found = True\n",
    "            question_type = text.lower()\n",
    "            break;\n",
    "    if (not found):\n",
    "        for text in tokenized_text:\n",
    "            if text.lower() in passiveQuestions:\n",
    "                found = True\n",
    "                question_type = text.lower()\n",
    "                break;\n",
    "    if (not found):\n",
    "        count = count + 1\n",
    "#         print (index,sentence,row[\"text\"])\n",
    "    print (index)\n",
    "    question_types.append(question_type)\n",
    "    \n",
    "df_testing['question_type'] = question_types\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get POS of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sentence in df_training['question']:\n",
    "    result = []\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    for text,pos in tagged:\n",
    "        text = lemmatize(text.lower())\n",
    "        if text not in stopwords:\n",
    "            if pos in POS:\n",
    "                result.append(pos)\n",
    "    results.append(result)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Answer from paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "count = 0\n",
    "for index,row in df_devel.iterrows():\n",
    "    p_id = row['answer_paragraph']\n",
    "    doc_id = row['docid']\n",
    "    paragraph = df_docs.iloc[doc_id]['text'][p_id]\n",
    "    found = False\n",
    "    for sent in punk_tokenizer.tokenize(paragraph):\n",
    "        if(answer_find(sent,row['text'])):\n",
    "            sentences.append(sent)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "#         print (index,row['text'])\n",
    "        for sent in punk_tokenizer.tokenize(paragraph):\n",
    "            if(answer_find2(sent, row['text'])):\n",
    "                sentences.append(sent)\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "#         print (index,row['text'])\n",
    "        for sent in nltk.sent_tokenize(paragraph):\n",
    "            if row['text'].lower() in sent.lower():\n",
    "                sentences.append(sent)\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        count = count + 1\n",
    "        print (index,row['text'])\n",
    "        print(paragraph)\n",
    "        sentences.append(None)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special answer tweeking trick that helped my project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "answer_type = []\n",
    "count  = 0\n",
    "for index,row in df_devel.iterrows():\n",
    "    nes = row['NE_text']\n",
    "    answer = row['text']\n",
    "    answer_entity = None\n",
    "    if(not row['answer_type']):\n",
    "        if(len(nes)==1):\n",
    "            answer_entity = tag\n",
    "        elif(len(nes)>1):\n",
    "            check = set()\n",
    "            for ne in nes:\n",
    "                check.add(ne[1])\n",
    "            if(len(check) == 1):\n",
    "                tag = list(check)[0]\n",
    "                answer_entity = tag\n",
    "            else:\n",
    "    #             print(nes)\n",
    "                for ne in nes:\n",
    "                    word= ne[0]\n",
    "                    tag = ne[1]\n",
    "                    seq=difflib.SequenceMatcher(None, word,answer)\n",
    "                    d=seq.ratio()*100\n",
    "                    if (d>60.00):\n",
    "    #                     print (\"1.1-\",index, word, answer,d,tag)\n",
    "                        answer_entity = tag\n",
    "                        break\n",
    "                    else:\n",
    "    #                     print (\"1.2-\",index, word, answer,d)\n",
    "                        tokenize_answer = answer.split(\" \")\n",
    "                        if(len(tokenize_answer)>1):\n",
    "                            for ans in tokenize_answer:\n",
    "                                seq=difflib.SequenceMatcher(None, word,ans)\n",
    "                                d=seq.ratio()*100\n",
    "                                if (d>30.00):\n",
    "                                    answer_entity = tag\n",
    "                                    break\n",
    "        else:\n",
    "    #         print(len(nes))\n",
    "            nes = row['NE_corrected_answer']\n",
    "            if(len(nes)==1):\n",
    "                answer_entity = tag\n",
    "            if(len(nes)>1):\n",
    "                check = set()\n",
    "                for ne in nes:\n",
    "                    check.add(ne[1])\n",
    "                if(len(check) == 1):\n",
    "                    tag = list(check)[0]\n",
    "                    answer_entity = tag\n",
    "                else:\n",
    "    #                 print(nes)\n",
    "                    for ne in nes:\n",
    "                        word= ne[0]\n",
    "                        tag = ne[1]\n",
    "                        seq=difflib.SequenceMatcher(None, word,answer)\n",
    "                        d=seq.ratio()*100\n",
    "                        if (d>60.00):\n",
    "    #                         print (\"2.1-\",index, word, answer,d)\n",
    "                            answer_entity = tag\n",
    "                            count = count + 1\n",
    "                            break\n",
    "                        else:\n",
    "    #                         print (\"2.2-\",index, word, answer,d)\n",
    "                            tokenize_answer = answer.split(\" \")\n",
    "                            if(len(tokenize_answer)>1):\n",
    "                                for ans in tokenize_answer:\n",
    "                                    seq=difflib.SequenceMatcher(None, word,ans)\n",
    "                                    d=seq.ratio()*100\n",
    "                                    if (d>30.00):\n",
    "        #                                 print(index,word,tag)\n",
    "                                        answer_entity = tag\n",
    "                                        count = count + 1\n",
    "                                        break\n",
    "        #                             else:\n",
    "        #                                 print(index,word,ans,d)\n",
    "        #                                 print(row['text'])\n",
    "        #     print (index, answer,answer_entity)\n",
    "        if(not answer_entity) and  (len(nes)>=1):\n",
    "            print(index, answer,answer_entity,nes)\n",
    "    else:\n",
    "        answer_entity = row['answer_type']\n",
    "    answer_type.append(answer_entity)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create dataset (No need to run now)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most common words from answer sentences (we can twek this for paragraph)\n",
    "answer_sentences_bow=get_sentences_bow(question_learning_dataset[question_learning_dataset['answer_found'].notnull()]['answer_found'])\n",
    "answer_keywords = set([word for word, count in answer_sentences_bow.items()])\n",
    "\n",
    "# Feature matrix\n",
    "qs_training=get_feature_questions(list(question_learning_dataset.question),answer_keywords,list(question_learning_dataset.question_type))\n",
    "qs_dev=get_feature_questions(list(question_devel_dataset.question),answer_keywords,list(question_devel_dataset.question_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Learning model Random Forest / Multinomial NB</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MultinomialNB(2, False, None)\n",
    "model=RandomForestClassifier(n_estimators = 300, max_depth = 60, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not vectorizer):\n",
    "    vectorizer = DictVectorizer()\n",
    "\n",
    "X_train_dtm = vectorizer.fit_transform(qs_training)\n",
    "X_dev_dtm = vectorizer.transform(qs_dev)\n",
    "\n",
    "model=RandomForestClassifier(n_estimators = 300, max_depth = 60, criterion = 'entropy')\n",
    "model.fit(X_train_dtm, list(question_learning_dataset.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_class = model.predict(X_dev_dtm)\n",
    "check_results(y_predicted_class,list(question_devel_dataset.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gensim Model (Finally not selected)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting words of corpus and learning the model (Already trained and no need to do again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for index,row in df_docs.iterrows():\n",
    "    for paragraph in row['text']:\n",
    "        tokenized_sentences = nltk.sent_tokenize(paragraph)\n",
    "        for sentence in tokenized_sentences:\n",
    "            tokenized_words = nltk.word_tokenize(sentence)\n",
    "            words = [lemmatize(x.lower()) for x in tokenized_words]\n",
    "            sentences.append(words)\n",
    "print(\"Done\")\n",
    "\n",
    "for index,row in df_training.iterrows():\n",
    "    question = row['question']\n",
    "    tokenized_word = nltk.word_tokenize(question)\n",
    "    words = [lemmatize(x.lower()) for x in tokenized_word]\n",
    "    sentences.append(words)\n",
    "print(\"Done\")\n",
    "\n",
    "for index,row in df_devel.iterrows():\n",
    "    question = row['question']\n",
    "    tokenized_word = nltk.word_tokenize(question)\n",
    "    words = [lemmatize(x.lower()) for x in tokenized_word]\n",
    "    sentences.append(words)\n",
    "print(\"Done\")\n",
    "\n",
    "print (\"It will take a while to learn the Gensim Word2Vec model. Please wait.\")\n",
    "gensim_model = Word2Vec(sentences, size=500, workers=4, iter = 50)\n",
    "print (model)\n",
    "gensim_model.save(\"gensim_model\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create features using gensim model to learn a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_features = []\n",
    "gensim_labels = []\n",
    "for index,row in question_learning_dataset.iterrows():\n",
    "    question = row['question']\n",
    "    gensim_features.append(sentence_vector(\n",
    "        model, question,vsm_inverted_index_corpus[row['docid']],row['answer_paragraph'],row['question_type']))\n",
    "    gensim_labels.append(row['answer_type'])\n",
    "    print (index)\n",
    "print (\"done\")\n",
    "\n",
    "save_obj(gensim_features,'features')\n",
    "save_obj(gensim_labels,'labels')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = gensim_features[0:22000]\n",
    "x_test = gensim_features[22000:24982]\n",
    "y_train = gensim_labels[0:22000]\n",
    "y_test = gensim_labels[22000:24982]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(random_state=42)\n",
    "param_grid = { \n",
    "    'n_estimators': [300],\n",
    "    'max_depth' : [40]\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 2,verbose = 4,n_jobs= -1)\n",
    "CV_rfc.fit(x_train, y_train)\n",
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_class = CV_rfc.predict(x_test)\n",
    "check_results(y_predicted_class,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Saving object if necessary</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(model,'random_forest')\n",
    "save_obj(vectorizer,'Vectorizer')\n",
    "save_obj(qs_training,'qs_training')\n",
    "save_obj(qs_dev,'qs_dev')\n",
    "save_obj(answer_keywords,'answer_keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For development Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_devel1=df_devel.iloc[0:10]\n",
    "count = 0\n",
    "for index, row in df_devel.iterrows():\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    ida=index\n",
    "    question_keywords=get_keyword(question)\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(2)\n",
    "    par_retrieved = False\n",
    "    possible_par=[par[0] for par in documents_ranked]\n",
    "    if(row['answer_paragraph'] in set(possible_par)):\n",
    "        par_retrieved = True\n",
    "        count  = count + 1\n",
    "    df_result_devel.loc[ida] = [docid,question,row['answer_paragraph'],\n",
    "                                possible_par,possible_par,\n",
    "                                par_retrieved,row['answer_found'],None,None,False,row['text'],None,\n",
    "                                row['NE_question'],row['label'],None]\n",
    "\n",
    "print(count*1.0/len(df_devel))\n",
    "\n",
    "df_result_devel = df_result_devel[df_result_devel['paragraph_found']==True]\n",
    "print(len(df_result_devel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing1=df_testing.iloc[0:10]\n",
    "count = 0\n",
    "for index, row in df_testing1.iterrows():\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    ida=index\n",
    "    question_keywords=get_keyword(question)\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(2)\n",
    "    possible_par=[par[0] for par in documents_ranked]\n",
    "    df_result_test.loc[ida] = [docid,question,None,\n",
    "                                possible_par,possible_par,\n",
    "                                None,None,None,None,False,None,None,\n",
    "                                row['NE_question'],None,None]\n",
    "    print(index)\n",
    "df_result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF calculation (Not needed. Saved object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation (Not needed. Saved object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_corpus_term_frquency = get_document_paragraphs_term(df_docs)\n",
    "doc_corpus_inverse_term_frquency = compute_paragraph_freqs(doc_corpus_term_frquency)\n",
    "\n",
    "vsm_inverted_index_corpus = defaultdict()\n",
    "for docid, doc_freq in doc_corpus_term_frquency.items():\n",
    "    vsm_inverted_index_doc = defaultdict(list)\n",
    "    N = count_words(doc_freq)\n",
    "    for paragraph_id, paragraph_freqs in doc_freq.items():\n",
    "        length = 0\n",
    "        tfidf_values = []\n",
    "        for term, count in paragraph_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(doc_corpus_inverse_term_frquency[docid][term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            vsm_inverted_index_doc[term].append([paragraph_id, tfidf / length])\n",
    "    vsm_inverted_index_corpus[docid] = vsm_inverted_index_doc\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(doc_corpus_term_frquency,'doc_corpus_term_frquency')\n",
    "save_obj(doc_corpus_inverse_term_frquency,'doc_corpus_inverse_term_frquency')\n",
    "save_obj(vsm_inverted_index_corpus,'vsm_inverted_index_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count  = 0\n",
    "for index,row in df_result_devel.iterrows():\n",
    "    NE_question = row['NE_question']\n",
    "    question = row['question']\n",
    "    answer = row['sentence']\n",
    "    docid = row['id']\n",
    "    candidate_answers = {}\n",
    "    possible_paragraphs = row['predicted_paragraph']\n",
    "#     print(possible_paragraphs)\n",
    "    sentences_check = Counter()\n",
    "    for paragraph_id in possible_paragraphs:\n",
    "        paragraph = df_docs.iloc[row['id']]['text'][paragraph_id]\n",
    "        candidate_answers[paragraph_id] = punk_tokenizer.tokenize(paragraph)\n",
    "        question_keywords=get_keyword(question)\n",
    "        for sentence_index,sentence in enumerate(candidate_answers[paragraph_id]):\n",
    "            NER_sentence=NER_corpus[docid][paragraph_id][sentence_index]\n",
    "            answer_sentence_keywords = get_keyword(sentence)\n",
    "            common_keywords=get_number_of_common_kewyords(question_keywords,answer_sentence_keywords)\n",
    "            common_entities = get_number_of_common_entities(NER_sentence,NE_question)\n",
    "\n",
    "            # longest exact sequence of keywords\n",
    "            longest_exact_sequence=0\n",
    "            for i in range(len(question_keywords)):\n",
    "                if i < len(answer_sentence_keywords):\n",
    "                    if question_keywords[i] in answer_sentence_keywords[i]:\n",
    "                        longest_exact_sequence+=1\n",
    "\n",
    "            # proximity\n",
    "            proximity=0\n",
    "            question_keywords_span=question_keywords.copy()\n",
    "            index_qk=0\n",
    "            while len(question_keywords_span)>0 and index_qk<len(question_keywords_span):\n",
    "                proximity+=1\n",
    "                if question_keywords_span[index_qk] in answer_sentence_keywords:\n",
    "                    question_keywords_span.pop(index_qk)\n",
    "                index_qk+=1\n",
    "\n",
    "            # n-gram overlap\n",
    "            bigrams_question =  nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(question)])\n",
    "            ngram_overlap=0\n",
    "\n",
    "            for bigram_question in bigrams_question:\n",
    "                bigrams_sentence = nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(sentence)])\n",
    "                for bigram_sentence in bigrams_sentence:\n",
    "                    if bigram_question == bigram_sentence:\n",
    "                        ngram_overlap+=1\n",
    "            \n",
    "            sentence_score = common_keywords + common_entities + longest_exact_sequence - proximity + ngram_overlap\n",
    "            df_best_sentences.loc[len(df_best_sentences)]=[row['id'],\n",
    "                                                       paragraph_id,row['paragraph'],sentence_index,\n",
    "                                                       sentence,sentence_score,row['sentence']]\n",
    "            sentences_check[(paragraph_id,sentence_index)] = sentence_score\n",
    "    \n",
    "    best = sentences_check.most_common()\n",
    "    df_result_devel.at[index, 'predicted_sentence_id'] = best[0][0][1]\n",
    "    df_result_devel.at[index, 'predicted_sentence'] = candidate_answers[best[0][0][0]][best[0][0][1]]\n",
    "    print(index)\n",
    "    if(row['sentence']==candidate_answers[best[0][0][0]][best[0][0][1]]):\n",
    "        count = count + 1\n",
    "        df_result_devel.at[index, 'sentence_found'] = True\n",
    "        df_result_devel.at[index, 'predicted_paragraph_final'] = best[0][0][0]\n",
    "print(count*1.0/len(df_result_devel))\n",
    "df_result_devel = df_result_devel[df_result_devel['sentence_found']==True]\n",
    "print(\"Sentences found :\",len(df_result_devel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count  = 0\n",
    "for index,row in df_result_test.iterrows():\n",
    "    NE_question = row['NE_question']\n",
    "    question = row['question']\n",
    "    docid = row['id']\n",
    "    candidate_answers = {}\n",
    "    possible_paragraphs = row['predicted_paragraph']\n",
    "    sentences_check = Counter()\n",
    "    if(len(possible_paragraphs)>0):\n",
    "        for paragraph_id in possible_paragraphs:\n",
    "            paragraph = df_docs.iloc[row['id']]['text'][paragraph_id]\n",
    "            candidate_answers[paragraph_id] = punk_tokenizer.tokenize(paragraph)\n",
    "            question_keywords=get_keyword(question)\n",
    "            for sentence_index,sentence in enumerate(candidate_answers[paragraph_id]):\n",
    "                NER_sentence=NER_corpus[docid][paragraph_id][sentence_index]\n",
    "                answer_sentence_keywords = get_keyword(sentence)\n",
    "                common_keywords=get_number_of_common_kewyords(question_keywords,answer_sentence_keywords)\n",
    "                common_entities = get_number_of_common_entities(NER_sentence,NE_question)\n",
    "\n",
    "                # longest exact sequence of keywords\n",
    "                longest_exact_sequence=0\n",
    "                for i in range(len(question_keywords)):\n",
    "                    if i < len(answer_sentence_keywords):\n",
    "                        if question_keywords[i] in answer_sentence_keywords[i]:\n",
    "                            longest_exact_sequence+=1\n",
    "\n",
    "                # proximity\n",
    "                proximity=0\n",
    "                question_keywords_span=question_keywords.copy()\n",
    "                index_qk=0\n",
    "                while len(question_keywords_span)>0 and index_qk<len(question_keywords_span):\n",
    "                    proximity+=1\n",
    "                    if question_keywords_span[index_qk] in answer_sentence_keywords:\n",
    "                        question_keywords_span.pop(index_qk)\n",
    "                    index_qk+=1\n",
    "\n",
    "                # n-gram overlap\n",
    "                bigrams_question =  nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(question)])\n",
    "                ngram_overlap=0\n",
    "\n",
    "                for bigram_question in bigrams_question:\n",
    "                    bigrams_sentence = nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(sentence)])\n",
    "                    for bigram_sentence in bigrams_sentence:\n",
    "                        if bigram_question == bigram_sentence:\n",
    "                            ngram_overlap+=1\n",
    "\n",
    "                sentence_score = common_keywords + common_entities + longest_exact_sequence - proximity + ngram_overlap\n",
    "                df_best_sentences.loc[len(df_best_sentences)]=[row['id'],\n",
    "                                                           paragraph_id,row['paragraph'],sentence_index,\n",
    "                                                           sentence,sentence_score,row['sentence']]\n",
    "                sentences_check[(paragraph_id,sentence_index)] = sentence_score\n",
    "\n",
    "        best = sentences_check.most_common()\n",
    "        df_result_test.at[index, 'predicted_paragraph_final'] = best[0][0][0]\n",
    "        df_result_test.at[index, 'predicted_sentence_id'] = best[0][0][1]\n",
    "        df_result_test.at[index, 'predicted_sentence'] = candidate_answers[best[0][0][0]][best[0][0][1]]\n",
    "    print(index)\n",
    "df_result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "df_result_devel['predicted_label'] = None\n",
    "df_result_devel['type_predicted'] = None\n",
    "\n",
    "for index,row in df_result_devel.iterrows():\n",
    "    sentence = row['predicted_sentence']\n",
    "    question = row['question']\n",
    "    question_NER = row['NE_question']\n",
    "    sentence_id = row['predicted_sentence_id']\n",
    "    paragraph_id = row['predicted_paragraph_final']\n",
    "    docid = row['id']\n",
    "    answer = row['answer']\n",
    "    sentence_NER = NER_corpus[docid][paragraph_id][sentence_id]\n",
    "    sentence_keywords = get_keyword(sentence)\n",
    "    question_keywords=get_keyword(question)\n",
    "    predicted_label = random_forest_model.predict(vectorizer.transform(\n",
    "        get_feature_question(question,answer_keywords,get_question_type(question))))[0]\n",
    "    \n",
    "    df_result_devel.at[index, 'predicted_label'] = predicted_label\n",
    "    \n",
    "    answer_type = None\n",
    "    predicted_answer = None\n",
    "    list_match_answer_type=[]\n",
    "    list_match_pred_answer=[]\n",
    "    first_match=True\n",
    "    for NE in sentence_NER:\n",
    "        if (label[NE[1]]==predicted_label):            \n",
    "            answer_type = NE[1]\n",
    "            predicted_answer = NE[0]\n",
    "            list_match_answer_type.append(answer_type)\n",
    "            list_match_pred_answer.append(predicted_answer)\n",
    "            if first_match:\n",
    "                break\n",
    "        else:\n",
    "            first_match=False\n",
    "    if (len(list_match_pred_answer)!=0):\n",
    "        print(index,\"NER best\",list_match_pred_answer)\n",
    "        df_result_devel.at[index,'type_predicted'] = \"best NER\"\n",
    "    if (len(list_match_pred_answer)==0):\n",
    "        for NE in sentence_NER:            \n",
    "            answer_type = NE[1]\n",
    "            predicted_answer = NE[0]\n",
    "            if(predicted_answer.lower() not in [x.lower() for x in list_match_pred_answer]):\n",
    "                list_match_answer_type.append(answer_type)\n",
    "                list_match_pred_answer.append(predicted_answer)\n",
    "        if (len(list_match_pred_answer)!=0):\n",
    "            print(index,\"NER\",list_match_pred_answer)\n",
    "            df_result_devel.at[index,'type_predicted'] = \"NER\"\n",
    "    if (len(list_match_pred_answer)==0):\n",
    "        for word,pos in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'CD'):\n",
    "#                 if (pos == 'NN'or pos == 'NNP' or pos == 'CD'):\n",
    "                if pos in POS:\n",
    "#                     if(word.lower() not in [x.lower() for x in list_match_pred_answer]):\n",
    "                    list_match_pred_answer.append(word)\n",
    "                    list_match_answer_type.append(pos)\n",
    "        if (len(list_match_pred_answer)!=0):\n",
    "            print(index,\"POS\",list_match_pred_answer)\n",
    "            df_result_devel.at[index,'type_predicted'] = \"POS\"\n",
    "    if (len(list_match_pred_answer)==0):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word not in stopwords:\n",
    "                if(word.lower() not in [x.lower() for x in list_match_pred_answer]):\n",
    "                    list_match_pred_answer.append(word)\n",
    "        if (len(list_match_pred_answer)!=0):\n",
    "            print(index,\"keyword\",list_match_pred_answer)\n",
    "            df_result_devel.at[index,'type_predicted'] = \"keyword\"\n",
    "\n",
    "    df_result_devel.at[index, 'predicted_answer'] = \" \".join(list_match_pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results(list(df_result_devel.predicted_label),list(df_result_devel.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Predicted Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index,row in df_result_devel.iterrows():\n",
    "    ans = row['label']\n",
    "    pred = row['predicted_label']\n",
    "    if(ans == pred):\n",
    "        count  = count + 1\n",
    "print(count/len(df_result_devel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_result_devel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "count4 = 0\n",
    "count5 = 0\n",
    "count6 = 0\n",
    "\n",
    "count7 = 0\n",
    "count8 = 0\n",
    "count9 = 0\n",
    "for index,row in df_result_devel.iterrows():\n",
    "    ans = row['answer']\n",
    "    pred = row['predicted_answer']\n",
    "    type_p = row['type_predicted']\n",
    "    if(type_p == 'best NER'):\n",
    "        count7 = count7 + 1\n",
    "    elif(type_p == 'NER'):\n",
    "        count8 = count8 + 1\n",
    "    else:\n",
    "        count9 = count9 + 1\n",
    "        \n",
    "    if(ans == pred):\n",
    "        count = count + 1\n",
    "    else:\n",
    "        if(pred):\n",
    "            seq=difflib.SequenceMatcher(None, ans,pred)\n",
    "            d=seq.ratio()*100\n",
    "            if (d>=80.00):\n",
    "#                 print(row['label'],\"~~\",row['predicted_label'],'~~',ans,\"~~\",pred)\n",
    "                count1 = count1 + 1\n",
    "            elif(d>60 and d<80.00):\n",
    "#                 print(row['label'],\"~~\",row['predicted_label'],'~~',ans,\"~~\",pred)\n",
    "                count2 = count2 + 1\n",
    "            elif(d>40 and d<=60.00):\n",
    "#                 print(row['label'],\"~~\",row['predicted_label'],'~~',ans,\"~~\",pred)\n",
    "                count3 = count3 + 1\n",
    "            elif(d>20 and d<40.00):\n",
    "#                 print(row['label'],\"~~\",row['predicted_label'],'~~',ans,\"~~\",pred)\n",
    "                count4 = count4 + 1\n",
    "            else:\n",
    "#                 print(row['label'],\"~~\",row['predicted_label'],'~~',ans,\"~~\",pred)\n",
    "                count5 = count5 + 1\n",
    "        else:\n",
    "            print(\"strange\",pred,row['answer'],\"~~\",row[\"predicted_answer\"])\n",
    "            count6 = count6 + 1\n",
    "            \n",
    "print(count/len(df_result_devel),count1/len(df_result_devel),count2/len(df_result_devel),count3/len(df_result_devel),count4/len(df_result_devel),count5/len(df_result_devel),count6/len(df_result_devel))\n",
    "cc = count + count1 + count2 + count3 + count4 + count5 + count6\n",
    "print(cc)\n",
    "print(count7/len(df_result_devel),count8/len(df_result_devel),count9/len(df_result_devel))\n",
    "print(count7,count8,count9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count10 = 0\n",
    "count11 = 0\n",
    "count12 = 0\n",
    "count13 = 0\n",
    "count14 = 0\n",
    "count15 = 0\n",
    "for index,row in df_result_devel.iterrows():\n",
    "    type_p = row['type_predicted']\n",
    "    actual = row['label']\n",
    "    predicted = row['predicted_label']\n",
    "    if(type_p == 'best NER'):\n",
    "        if(actual==predicted):\n",
    "            count10 = count10 + 1\n",
    "        else:\n",
    "            count11 = count11+1\n",
    "    elif(type_p == 'NER'):\n",
    "        if(actual==predicted):\n",
    "            count12 = count12 + 1\n",
    "        else:\n",
    "            count13 = count13+1\n",
    "    else:\n",
    "        if(actual==predicted):\n",
    "            count14 = count14 + 1\n",
    "        else:\n",
    "            count15 = count15+1\n",
    "print(count10,count11,count12,count13,count14,count15)\n",
    "print(count10/1031,count11,count12/407,count13,count14/108,count15)\n",
    "total = count10 + count11 + count12 + count13 + count14 + count15\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result=pd.DataFrame(columns=['id','answer'])\n",
    "df_result_test['predicted_label'] = None\n",
    "for index,row in df_result_test.iterrows():\n",
    "#     print(index)\n",
    "    sentence = row['predicted_sentence']\n",
    "    question = row['question']\n",
    "    question_NER = row['NE_question']\n",
    "    sentence_id = row['predicted_sentence_id']\n",
    "    paragraph_id = row['predicted_paragraph_final']\n",
    "    docid = row['id']\n",
    "    if(paragraph_id):\n",
    "        sentence_NER = NER_corpus[docid][paragraph_id][sentence_id]\n",
    "        sentence_keywords = get_keyword(sentence)\n",
    "        question_keywords=get_keyword(question)\n",
    "        predicted_label = random_forest_model.predict(vectorizer.transform(\n",
    "            get_feature_question(question,answer_keywords,get_question_type(question))))[0]\n",
    "\n",
    "        df_result_test.at[index, 'predicted_label'] = predicted_label\n",
    "\n",
    "        answer_type = None\n",
    "        predicted_answer = None\n",
    "        list_match_answer_type=[]\n",
    "        list_match_pred_answer=[]\n",
    "        first_match=True\n",
    "        for NE in sentence_NER:\n",
    "            if (label[NE[1]]==predicted_label):            \n",
    "                answer_type = NE[1]\n",
    "                predicted_answer = NE[0]\n",
    "                list_match_answer_type.append(answer_type)\n",
    "                list_match_pred_answer.append(predicted_answer)\n",
    "                if first_match:\n",
    "                    break\n",
    "            else:\n",
    "                first_match=False\n",
    "        if (len(list_match_pred_answer)!=0):\n",
    "            print(index,\"NER best\",list_match_pred_answer)\n",
    "        if (len(list_match_pred_answer)==0):\n",
    "            for NE in sentence_NER:            \n",
    "                answer_type = NE[1]\n",
    "                predicted_answer = NE[0]\n",
    "                if(predicted_answer.lower() not in [x.lower() for x in list_match_pred_answer]):\n",
    "                    list_match_answer_type.append(answer_type)\n",
    "                    list_match_pred_answer.append(predicted_answer)\n",
    "            if (len(list_match_pred_answer)!=0):\n",
    "                print(index,\"NER\",list_match_pred_answer)\n",
    "        if (len(list_match_pred_answer)==0):\n",
    "            for word,pos in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "                if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'CD'):\n",
    "#                 if (pos == 'NN'or pos == 'NNP' or pos == 'CD'):\n",
    "                    if pos in POS:\n",
    "#                     if(word.lower() not in [x.lower() for x in list_match_pred_answer]):\n",
    "                        list_match_pred_answer.append(word)\n",
    "                        list_match_answer_type.append(pos)\n",
    "            if (len(list_match_pred_answer)!=0):\n",
    "                print(index,\"POS\",list_match_pred_answer)\n",
    "        if (len(list_match_pred_answer)==0):\n",
    "            for word in nltk.word_tokenize(sentence):\n",
    "                if word not in stopwords:\n",
    "                    if(word.lower() not in [x.lower() for x in list_match_pred_answer]):\n",
    "                        list_match_pred_answer.append(word)\n",
    "            if (len(list_match_pred_answer)!=0):\n",
    "                print(index,\"keyword\",list_match_pred_answer)\n",
    "\n",
    "        df_result_test.at[index, 'predicted_answer'] = \" \".join(list_match_pred_answer)\n",
    "        df_result.loc[len(df_result)]=[index,\" \".join(list_match_pred_answer)]\n",
    "    else:\n",
    "        df_result.loc[len(df_result)]=[index,\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('prediction/output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(question_devel_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
